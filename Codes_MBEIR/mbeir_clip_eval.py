import sys
import os
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pickle
import gc
import json
import numpy as np
from PIL import Image
import logging
import time
from typing import Dict, List, Union, Tuple
from datasets import load_dataset

# è®¾ç½®å†…å­˜ä¼˜åŒ–
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

class MBEIRCLIPEvaluator:
    """M-BEIRæ•°æ®é›†çš„CLIPè¯„ä¼°å™¨"""
    
    def __init__(self, clip_type: str = "CLIP", device: str = "cuda"):
        """
        åˆå§‹åŒ–M-BEIR CLIPè¯„ä¼°å™¨
        
        Args:
            clip_type: "CLIP" æˆ– "Long-CLIP"
            device: è®¾å¤‡åç§°
        """
        self.clip_type = clip_type
        self.device = device
        self.clip_model = None
        self.preprocess = None
        self.tokenizer = None
        
        self._init_clip_model()
    
    def _init_clip_model(self):
        """åˆå§‹åŒ–CLIPæ¨¡å‹"""
        print(f"æ­£åœ¨åˆå§‹åŒ– {self.clip_type} æ¨¡å‹...")

        self.clear_cache()
        
        # åŠ è½½SEARLEæ¨¡å‹
        clip_model_name = "ViT-L/14"
        self.searle_model, self.encode_with_pseudo_tokens = torch.hub.load(
            repo_or_dir='/root/dws/MCS/SEARLE', 
            source='local', 
            model='searle',
            backbone=clip_model_name
        )
        self.searle_model.to(self.device)
        if self.clip_type == "Long-CLIP":
            sys.path.insert(0, '/root/dws/MCS/Long-CLIP')
            sys.path.insert(0, '/root/dws/MCS/Long-CLIP/model')
            from model import longclip as clip # type: ignore
            self.clip_model, self.preprocess = clip.load(
                "/root/dws/MCS/Long-CLIP/checkpoints/longclip-L.pt",
                device=self.device
            )
            self.tokenizer = clip.tokenize
            
        elif self.clip_type == "CLIP":
            sys.path.insert(0, '/root/dws/MCS/CLIP')
            import clip
            self.clip_model, self.preprocess = clip.load(
                "ViT-L/14@336px",
                device=self.device
            )
            self.tokenizer = clip.tokenize
        
        print(f"{self.clip_type} æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        self.clear_cache()
    
    def clear_cache(self):
        """æ¸…ç†GPUå’Œç³»ç»Ÿç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
    
    def extract_query_features(self, query_text: str, query_image_path: str = None, 
                              img_dir: str = None) -> torch.Tensor:
        """
        æå–æŸ¥è¯¢ç‰¹å¾
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_image_path: æŸ¥è¯¢å›¾åƒè·¯å¾„
            img_dir: å›¾åƒç›®å½•
            
        Returns:
            å½’ä¸€åŒ–çš„æŸ¥è¯¢ç‰¹å¾å‘é‡
        """
        with torch.no_grad():
            features = []
            if query_text and query_text.strip() and query_image_path and query_image_path.strip() and img_dir:
                # å¤„ç†å›¾åƒè·¯å¾„
                img_path = os.path.join(img_dir, query_image_path)
                if os.path.exists(img_path):
                    try:
                        image = Image.open(img_path).convert('RGB')
                        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
                        image_features = self.clip_model.encode_image(image_input).float()
                        pseudo_tokens = self.searle_model(image_features.to(self.device))
                    except Exception as e:
                        print(f"å›¾åƒåŠ è½½å¤±è´¥: {img_path}, é”™è¯¯: {e}")
                text_input = self.tokenizer([query_text.strip()], truncate=True).to(self.device)
                
                query_feature = self.encode_with_pseudo_tokens(self.clip_model, text_input, pseudo_tokens)
                features.append(F.normalize(query_feature, dim=-1))
            # å¤„ç†æ–‡æœ¬
            elif query_text and query_text.strip():
                text_input = self.tokenizer([query_text.strip()], truncate=True).to(self.device)
                text_features = self.clip_model.encode_text(text_input).float()
                features.append(F.normalize(text_features, dim=-1))
            
            # å¤„ç†å›¾åƒ
            elif query_image_path and query_image_path.strip() and img_dir:
                img_path = os.path.join(img_dir, query_image_path)
                if os.path.exists(img_path):
                    try:
                        image = Image.open(img_path).convert('RGB')
                        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
                        image_features = self.clip_model.encode_image(image_input).float()
                        features.append(F.normalize(image_features, dim=-1))
                    except Exception as e:
                        print(f"å›¾åƒåŠ è½½å¤±è´¥: {img_path}, é”™è¯¯: {e}")
            
            # åˆå¹¶ç‰¹å¾
            # if len(features) == 2:
            #     # å›¾åƒ+æ–‡æœ¬ï¼šå–å¹³å‡
            #     combined_features = features[0] + features[1]
            #     return F.normalize(combined_features, dim=-1)
            # elif len(features) == 1:
            #     return features[0]
            # else:
            #     # è¿”å›é›¶å‘é‡
            #     feature_dim = 768
            #     return torch.zeros(1, feature_dim).to(self.device)
            if len(features) == 1:
                return features[0]
            else:
                # è¿”å›é›¶å‘é‡
                feature_dim = 768
                return torch.zeros(1, feature_dim).to(self.device)
    
    def extract_candidate_features(self, candidate_text: str, candidate_image_path: str = None,
                                 img_dir: str = None) -> torch.Tensor:
        """
        æå–å€™é€‰ç‰¹å¾
        
        Args:
            candidate_text: å€™é€‰æ–‡æœ¬
            candidate_image_path: å€™é€‰å›¾åƒè·¯å¾„
            img_dir: å›¾åƒç›®å½•
            
        Returns:
            å½’ä¸€åŒ–çš„å€™é€‰ç‰¹å¾å‘é‡
        """
        return self.extract_query_features(candidate_text, candidate_image_path, img_dir)
    
    def extract_batch_features(self, items: List[Dict], img_dir: str, 
                              text_key: str, image_key: str, 
                              batch_size: int = 32) -> torch.Tensor:

        all_features = []
        
        with torch.no_grad():
            for batch_start in tqdm(range(0, len(items), batch_size), desc="æ‰¹å¤„ç†æå–ç‰¹å¾"):
                batch_end = min(batch_start + batch_size, len(items))
                batch_items = items[batch_start:batch_end]
                
                # åˆ†ç¦»æ–‡æœ¬å’Œå›¾åƒæ•°æ®
                batch_texts = []
                batch_images = []
                batch_has_text = []
                batch_has_image = []
                
                for item in batch_items:
                    text = item.get(text_key, '')
                    image_path = item.get(image_key, '')
                    
                    # å¤„ç†æ–‡æœ¬
                    if text and text.strip():
                        batch_texts.append(text.strip())
                        batch_has_text.append(True)
                    else:
                        batch_texts.append('')
                        batch_has_text.append(False)
                    
                    # å¤„ç†å›¾åƒ
                    has_valid_image = False
                    if image_path and image_path.strip() and img_dir:
                        img_path = os.path.join(img_dir, image_path)
                        if os.path.exists(img_path):
                            try:
                                image = Image.open(img_path).convert('RGB')
                                batch_images.append(image)
                                has_valid_image = True
                            except Exception as e:
                                print(f"å›¾åƒåŠ è½½å¤±è´¥: {img_path}, é”™è¯¯: {e}")
                    
                    batch_has_image.append(has_valid_image)
                    if not has_valid_image:
                        batch_images.append(None)
                
                # æ‰¹é‡ç¼–ç æ–‡æœ¬
                text_features_batch = None
                if any(batch_has_text):
                    valid_texts = [text for text, has_text in zip(batch_texts, batch_has_text) if has_text]
                    if valid_texts:
                        text_tokens = self.tokenizer(valid_texts, truncate=True).to(self.device)
                        text_features_batch = self.clip_model.encode_text(text_tokens).float()
                        text_features_batch = F.normalize(text_features_batch, dim=-1)
                
                # æ‰¹é‡ç¼–ç å›¾åƒ
                image_features_batch = None
                if any(batch_has_image):
                    valid_images = [img for img, has_img in zip(batch_images, batch_has_image) if has_img]
                    if valid_images:
                        image_inputs = torch.stack([self.preprocess(img) for img in valid_images]).to(self.device)
                        image_features_batch = self.clip_model.encode_image(image_inputs).float()
                        image_features_batch = F.normalize(image_features_batch, dim=-1)
                
                # åˆå¹¶ç‰¹å¾
                batch_features = []
                text_idx = 0
                image_idx = 0
                
                for i in range(len(batch_items)):
                    features = []
                    
                    # æ·»åŠ æ–‡æœ¬ç‰¹å¾
                    if batch_has_text[i] and text_features_batch is not None:
                        features.append(text_features_batch[text_idx:text_idx+1])
                        text_idx += 1
                    
                    # æ·»åŠ å›¾åƒç‰¹å¾
                    if batch_has_image[i] and image_features_batch is not None:
                        features.append(image_features_batch[image_idx:image_idx+1])
                        image_idx += 1
                    
                    # åˆå¹¶ç‰¹å¾
                    if len(features) == 2:
                        # å¤šæ¨¡æ€ï¼šç›¸åŠ åå½’ä¸€åŒ–
                        # combined = features[0] + features[1]
                        combined = features[1]
                        final_feature = F.normalize(combined, dim=-1)
                    elif len(features) == 1:
                        final_feature = features[0]
                    else:
                        # é›¶å‘é‡
                        feature_dim = 768  # æ ¹æ®æ¨¡å‹è°ƒæ•´
                        final_feature = torch.zeros(1, feature_dim).to(self.device)
                    
                    batch_features.append(final_feature.cpu().float())
                
                all_features.extend(batch_features)
                
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        return torch.cat(all_features, dim=0)

def load_mbeir_data(data_dir: str, task_name: str, split: str = "test"):
    """
    åŠ è½½M-BEIRæ•°æ®é›†
    
    Args:
        data_dir: M-BEIRæ•°æ®ç›®å½•è·¯å¾„
        task_name: ä»»åŠ¡åç§°ï¼Œå¦‚ "mbeir_mscoco_task0"
        split: æ•°æ®åˆ†å‰²ï¼Œ"test", "val" ç­‰
    
    Returns:
        queries, candidates
    """
    # æŸ¥è¯¢æ–‡ä»¶è·¯å¾„
    query_file = os.path.join(data_dir, "query", split, f"{task_name}_{split}.jsonl")
    
    # å€™é€‰æ± æ–‡ä»¶è·¯å¾„
    if "mscoco" in task_name:
        cand_file = os.path.join(data_dir, "cand_pool", "local", f"{task_name}_{split}_cand_pool.jsonl")
    else:
        cand_file = os.path.join(data_dir, "cand_pool", "local", f"{task_name}_cand_pool.jsonl")
    
    # è¯»å–æŸ¥è¯¢æ•°æ®
    queries = []
    with open(query_file, 'r', encoding='utf-8') as f:
        for line in f:
            queries.append(json.loads(line.strip()))
    
    # è¯»å–å€™é€‰æ•°æ®
    candidates = []
    with open(cand_file, 'r', encoding='utf-8') as f:
        for line in f:
            candidates.append(json.loads(line.strip()))
    
    print(f"åŠ è½½å®Œæˆ: {len(queries)} ä¸ªæŸ¥è¯¢, {len(candidates)} ä¸ªå€™é€‰")
    return queries, candidates

def extract_query_embeddings(evaluator: MBEIRCLIPEvaluator, 
                           queries: List[Dict], 
                           img_dir: str,
                           save_path: str,
                           batch_size: int = 32) -> Tuple[torch.Tensor, List]:
    """æå–æŸ¥è¯¢åµŒå…¥ - æ‰¹å¤„ç†ç‰ˆæœ¬"""
    print(f"å¼€å§‹æ‰¹å¤„ç†æå– {len(queries)} ä¸ªæŸ¥è¯¢çš„åµŒå…¥ (æ‰¹å¤§å°: {batch_size})")
    
    # ä½¿ç”¨æ‰¹å¤„ç†æå–ç‰¹å¾
    embeddings = evaluator.extract_batch_features(
        items=queries,
        img_dir=img_dir,
        text_key='query_txt',
        image_key='query_img_path',
        batch_size=batch_size
    )
    
    # æ„å»ºå…ƒæ•°æ®
    metadata = []
    for item in queries:
        metadata.append({
            'qid': item.get('qid', ''),
            'query_txt': item.get('query_txt', ''),
            'query_img_path': item.get('query_img_path', ''),
            'query_modality': item.get('query_modality', ''),
            'task_id': item.get('task_id', ''),
            'pos_cand_list': item.get('pos_cand_list', []),
            'neg_cand_list': item.get('neg_cand_list', [])
        })
    
    # ä¿å­˜åµŒå…¥
    with open(save_path, 'wb') as f:
        pickle.dump((embeddings, metadata), f)
    
    print(f"æŸ¥è¯¢åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}, å½¢çŠ¶: {embeddings.shape}")
    return embeddings, metadata

def extract_candidate_embeddings(evaluator: MBEIRCLIPEvaluator,
                               candidates: List[Dict],
                               img_dir: str,
                               save_path: str,
                               batch_size: int = 32) -> Tuple[torch.Tensor, List]:
    """æå–å€™é€‰åµŒå…¥ - æ‰¹å¤„ç†ç‰ˆæœ¬"""
    print(f"å¼€å§‹æ‰¹å¤„ç†æå– {len(candidates)} ä¸ªå€™é€‰çš„åµŒå…¥ (æ‰¹å¤§å°: {batch_size})")
    
    # ä½¿ç”¨æ‰¹å¤„ç†æå–ç‰¹å¾
    embeddings = evaluator.extract_batch_features(
        items=candidates,
        img_dir=img_dir,
        text_key='txt',
        image_key='img_path',
        batch_size=batch_size
    )
    
    # æ„å»ºå…ƒæ•°æ®
    metadata = []
    for idx, candidate in enumerate(candidates):
        metadata.append({
            'did': candidate.get('did', ''),
            'txt': candidate.get('txt', ''),
            'img_path': candidate.get('img_path', ''),
            'modality': candidate.get('modality', ''),
            'candidate_idx': idx
        })
    
    # ä¿å­˜åµŒå…¥
    with open(save_path, 'wb') as f:
        pickle.dump((embeddings, metadata), f)
    
    print(f"å€™é€‰åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}, å½¢çŠ¶: {embeddings.shape}")
    return embeddings, metadata


def evaluate_mbeir_retrieval(query_embeddings: torch.Tensor,
                           candidate_embeddings: torch.Tensor,
                           query_metadata: List,
                           candidate_metadata: List,
                           k_values: List[int] = [1, 5, 10, 20],
                           batch_size: int = 100) -> Dict[str, float]:

    print(f"æŸ¥è¯¢æ•°é‡: {len(query_embeddings)}, å€™é€‰æ•°é‡: {len(candidate_embeddings)}")
    print(f"ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # é¢„å¤„ç†ï¼šåˆ›å»ºå€™é€‰IDæ•°ç»„å’ŒæŸ¥è¯¢æ­£ç¡®ç­”æ¡ˆé›†åˆ
    print("é¢„å¤„ç†æ•°æ®...")
    candidate_dids = np.array([cand_meta['did'] for cand_meta in candidate_metadata])
    
    # åªå¤„ç†æœ‰æ­£ç¡®ç­”æ¡ˆçš„æŸ¥è¯¢
    valid_queries = []
    query_pos_sets = []
    for i, query_meta in enumerate(query_metadata):
        pos_list = query_meta.get('pos_cand_list', [])
        if pos_list:
            valid_queries.append(i)
            query_pos_sets.append(set(pos_list))
    
    print(f"æœ‰æ•ˆæŸ¥è¯¢æ•°é‡: {len(valid_queries)}/{len(query_metadata)}")
    
    # è®¡ç®—æœ€å¤§kå€¼ï¼Œä¸€æ¬¡æ€§è®¡ç®—top-k
    max_k = max(k_values)
    max_k = min(max_k, len(candidate_embeddings))
    
    results = {}
    all_top_k_indices = []
    
    # æ£€æŸ¥æ•°æ®è§„æ¨¡å¹¶é€‰æ‹©è®¡ç®—ç­–ç•¥
    num_queries = len(query_embeddings)
    num_candidates = len(candidate_embeddings)
    total_pairs = num_queries * num_candidates
    
    print("è®¡ç®—ç›¸ä¼¼åº¦å’Œtop-kç´¢å¼•...")
    print(f"æ€»è®¡ç®—é‡: {num_queries} Ã— {num_candidates} = {total_pairs:,} ä¸ªç›¸ä¼¼åº¦å€¼")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ä¼˜åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šä½¿ç”¨çŸ©é˜µä¹˜æ³•ä»£æ›¿cosine_similarity
    # å…ˆå½’ä¸€åŒ–åµŒå…¥å‘é‡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å½’ä¸€åŒ–ï¼‰
    query_embeddings = F.normalize(query_embeddings, dim=-1)
    candidate_embeddings = F.normalize(candidate_embeddings, dim=-1)
    
    # å¯¹äºè¶…å¤§æ•°æ®é›†ï¼Œè€ƒè™‘ä½¿ç”¨å€™é€‰åˆ†å—ç­–ç•¥
    use_candidate_chunking = total_pairs > 50000000  # è¶…è¿‡5åƒä¸‡å¯¹æ—¶å¯ç”¨å€™é€‰åˆ†å—
    candidate_chunk_size = 50000 if use_candidate_chunking else len(candidate_embeddings)
    
    if use_candidate_chunking:
        print(f"æ•°æ®é›†è¿‡å¤§ï¼Œå¯ç”¨å€™é€‰åˆ†å—ç­–ç•¥ï¼Œå€™é€‰åˆ†å—å¤§å°: {candidate_chunk_size}")
    
    # åˆ†æ‰¹è®¡ç®—ç›¸ä¼¼åº¦å’Œtop-kç´¢å¼•ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
    num_batches = (len(query_embeddings) + batch_size - 1) // batch_size
    for batch_idx, batch_start in enumerate(tqdm(range(0, len(query_embeddings), batch_size), 
                           desc="è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ")):
        batch_end = min(batch_start + batch_size, len(query_embeddings))
        batch_query_embeddings = query_embeddings[batch_start:batch_end]
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´
        if batch_idx > 0:
            elapsed_time = time.time() - start_time
            avg_time_per_batch = elapsed_time / batch_idx
            remaining_batches = num_batches - batch_idx
            estimated_remaining_time = avg_time_per_batch * remaining_batches
            
        batch_end = min(batch_start + batch_size, len(query_embeddings))
        batch_query_embeddings = query_embeddings[batch_start:batch_end]
        
        if use_candidate_chunking:
            # å¯¹å€™é€‰ä¹Ÿè¿›è¡Œåˆ†å—å¤„ç†
            batch_similarities = []
            batch_indices = []
            
            for cand_start in range(0, len(candidate_embeddings), candidate_chunk_size):
                cand_end = min(cand_start + candidate_chunk_size, len(candidate_embeddings))
                cand_chunk = candidate_embeddings[cand_start:cand_end]
                
                with torch.no_grad():
                    # ä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ›´é«˜æ•ˆï¼‰
                    chunk_similarity = torch.mm(batch_query_embeddings, cand_chunk.T)
                    batch_similarities.append(chunk_similarity.cpu())
                    
                    # æ¸…ç†GPUå†…å­˜
                    del chunk_similarity
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
            
            # åˆå¹¶æ‰€æœ‰å€™é€‰å—çš„ç»“æœï¼Œæ‰¾åˆ°å…¨å±€top-k
            with torch.no_grad():
                full_similarity = torch.cat(batch_similarities, dim=1)
                _, batch_top_k_indices = torch.topk(full_similarity, k=max_k, dim=-1)
                all_top_k_indices.append(batch_top_k_indices.numpy())
                
                # æ¸…ç†å†…å­˜
                del batch_similarities, full_similarity, batch_top_k_indices
                
        else:
            # å¸¸è§„å¤„ç†
            with torch.no_grad():
                # ä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ›´é«˜æ•ˆï¼‰
                # ç›¸ä¼¼åº¦ = æŸ¥è¯¢ @ å€™é€‰.T
                similarity_matrix = torch.mm(batch_query_embeddings, candidate_embeddings.T)
                
                # ä¸€æ¬¡æ€§è®¡ç®—top-max_kç´¢å¼•
                _, batch_top_k_indices = torch.topk(similarity_matrix, k=max_k, dim=-1)
                all_top_k_indices.append(batch_top_k_indices.cpu().numpy())
                
                # æ¸…ç†å†…å­˜
                del similarity_matrix, batch_top_k_indices
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„top-kç´¢å¼•
    all_top_k_indices = np.concatenate(all_top_k_indices, axis=0)
    
    total_time = time.time() - start_time
    # print(f"ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼Œæ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    # print(f"å¹³å‡æ¯æ‰¹æ¬¡è€—æ—¶: {total_time/num_batches:.2f} ç§’")
    
    print("è®¡ç®—å„kå€¼çš„recallæŒ‡æ ‡...")
    # ç°åœ¨ä¸ºæ¯ä¸ªkå€¼è®¡ç®—recallï¼ˆé‡ç”¨top-kç´¢å¼•ï¼‰
    for k in k_values:
        current_k = min(k, max_k)
        correct = 0
        
        # æ‰¹é‡å¤„ç†æœ‰æ•ˆæŸ¥è¯¢
        for idx in tqdm(range(len(valid_queries)), desc=f"è®¡ç®—Recall@{k}", leave=False):
            query_idx = valid_queries[idx]
            pos_set = query_pos_sets[idx]
            
            # è·å–å½“å‰æŸ¥è¯¢çš„top-kå€™é€‰IDï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„ç´¢å¼•ï¼‰
            top_k_cand_indices = all_top_k_indices[query_idx, :current_k]
            top_k_cand_dids = candidate_dids[top_k_cand_indices]
            
            # ä½¿ç”¨é›†åˆäº¤é›†å¿«é€Ÿæ£€æŸ¥å‘½ä¸­
            if pos_set.intersection(top_k_cand_dids):
                correct += 1
        
        total = len(valid_queries)
        recall_at_k = correct / total if total > 0 else 0.0
        results[f'Recall@{k}'] = recall_at_k
        print(f"Recall@{k}: {recall_at_k:.4f} ({correct}/{total})")
    
    return results

def get_topk_candidates_for_rerank(query_embeddings: torch.Tensor,
                                 candidate_embeddings: torch.Tensor,
                                 query_metadata: List,
                                 candidate_metadata: List,
                                 queries: List[Dict],
                                 candidates: List[Dict],
                                 img_dir: str,
                                 k: int = 5,
                                 batch_size: int = 100) -> List[Dict]:
    print(f"å¼€å§‹è·å–æ¯ä¸ªæŸ¥è¯¢çš„top-{k}å€™é€‰é¡¹...")
    
    # å½’ä¸€åŒ–åµŒå…¥å‘é‡
    query_embeddings = F.normalize(query_embeddings, dim=-1)
    candidate_embeddings = F.normalize(candidate_embeddings, dim=-1)
    
    results = []
    
    # åˆ†æ‰¹å¤„ç†æŸ¥è¯¢
    num_batches = (len(query_embeddings) + batch_size - 1) // batch_size
    for batch_idx in tqdm(range(num_batches), desc="è·å–top-kå€™é€‰"):
        batch_start = batch_idx * batch_size
        batch_end = min(batch_start + batch_size, len(query_embeddings))
        batch_query_embeddings = query_embeddings[batch_start:batch_end]
        
        with torch.no_grad():
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = torch.mm(batch_query_embeddings, candidate_embeddings.T)
            
            # è·å–top-kç´¢å¼•å’Œåˆ†æ•°
            similarities, batch_top_k_indices = torch.topk(similarity_matrix, k=k, dim=-1)
            batch_top_k_indices = batch_top_k_indices.cpu().numpy()
            similarities = similarities.cpu().numpy()
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ¯ä¸ªæŸ¥è¯¢
            for i, query_idx in enumerate(range(batch_start, batch_end)):
                query_meta = query_metadata[query_idx]
                query_data = queries[query_idx]
                
                # è·å–top-kå€™é€‰ç´¢å¼•å’Œåˆ†æ•°
                top_k_indices = batch_top_k_indices[i]
                top_k_scores = similarities[i]
                
                # æ„å»ºå€™é€‰é¡¹åˆ—è¡¨
                top_k_candidates = []
                for rank, (cand_idx, score) in enumerate(zip(top_k_indices, top_k_scores)):
                    cand_meta = candidate_metadata[cand_idx]
                    cand_data = candidates[cand_idx]
                    
                    # æ„å»ºå€™é€‰é¡¹ä¿¡æ¯
                    candidate_info = {
                        'rank': rank + 1,
                        'candidate_idx': int(cand_idx),
                        'did': cand_meta['did'],
                        'txt': cand_meta['txt'],
                        'img_path': cand_meta['img_path'],
                        'modality': cand_meta['modality'],
                        'similarity_score': float(score),
                        # æ·»åŠ å®Œæ•´å›¾åƒè·¯å¾„
                        'full_img_path': os.path.join(img_dir, cand_meta['img_path']) if cand_meta['img_path'] else None
                    }
                    top_k_candidates.append(candidate_info)
                
                # æ„å»ºæŸ¥è¯¢ç»“æœ
                query_result = {
                    'query_idx': query_idx,
                    'qid': query_meta['qid'],
                    'query_txt': query_meta['query_txt'],
                    'query_img_path': query_meta['query_img_path'],
                    'query_modality': query_meta['query_modality'],
                    'full_query_img_path': os.path.join(img_dir, query_meta['query_img_path']) if query_meta['query_img_path'] else None,
                    'pos_cand_list': query_meta['pos_cand_list'],
                    'top_k_candidates': top_k_candidates
                }
                
                results.append(query_result)
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    print(f"æˆåŠŸè·å– {len(results)} ä¸ªæŸ¥è¯¢çš„top-{k}å€™é€‰é¡¹")
    return results

def save_topk_for_rerank(topk_results: List[Dict], save_path: str):
    """
    ä¿å­˜top-kç»“æœç”¨äºrerank
    
    Args:
        topk_results: top-kç»“æœåˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
    """
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(topk_results, f, indent=2, ensure_ascii=False)
    print(f"Top-kç»“æœå·²ä¿å­˜åˆ°: {save_path}")

def load_topk_for_rerank(load_path: str) -> List[Dict]:
    """
    åŠ è½½top-kç»“æœç”¨äºrerank
    
    Args:
        load_path: åŠ è½½è·¯å¾„
        
    Returns:
        top-kç»“æœåˆ—è¡¨
    """
    with open(load_path, 'r', encoding='utf-8') as f:
        topk_results = json.load(f)
    print(f"å·²ä» {load_path} åŠ è½½ {len(topk_results)} ä¸ªæŸ¥è¯¢çš„top-kç»“æœ")
    return topk_results

def calculate_rerank_accuracy(reranked_results: List[Dict], k_values: List[int] = [1]) -> Dict[str, float]:
    
    results = {}

    
    return results

def calculate_recall_from_results(results: List[Dict]) -> float:
    """ä»ç»“æœè®¡ç®—å¬å›ç‡"""
    total_samples = len(results)
    if total_samples == 0:
        return 0.0
    correct_count = sum(1 for res in results if res['is_correct'])
    return correct_count / total_samples

def evaluate_single_task(task_name: str, clip_type: str, device: str, 
                        mbeir_dir: str, img_dir: str, split: str = "test") -> Dict[str, float]:
    """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
    print(f"\n{'='*50}")
    print(f"å¼€å§‹è¯„ä¼°ä»»åŠ¡: {task_name}")
    print(f"CLIPæ¨¡å‹: {clip_type}")
    print(f"{'='*50}")
    
    save_dir = f"/root/dws/MCS/Codes_MBEIR/embeddings/{task_name}"
    os.makedirs(save_dir, exist_ok=True)
    
    # è®¾ç½®ä»»åŠ¡ç‰¹å®šçš„æ—¥å¿—
    log_path = os.path.join(save_dir, f"{task_name}_{clip_type}_evaluation.log")
    
    # åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„logger
    task_logger = logging.getLogger(task_name)
    task_logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ä¹‹å‰çš„handlers
    for handler in task_logger.handlers[:]:
        task_logger.removeHandler(handler)
    
    # æ·»åŠ æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_path, mode='w')
    file_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    task_logger.addHandler(file_handler)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = MBEIRCLIPEvaluator(clip_type=clip_type, device=device)
    
    try:
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        query_file = os.path.join(mbeir_dir, "query", split, f"{task_name}_{split}.jsonl")
        if "mscoco" in task_name:
            cand_file = os.path.join(mbeir_dir, "cand_pool", "local", f"{task_name}_{split}_cand_pool.jsonl")
        else:
            cand_file = os.path.join(mbeir_dir, "cand_pool", "local", f"{task_name}_cand_pool.jsonl")
        
        if not os.path.exists(query_file):
            print(f"âŒ æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {query_file}")
            task_logger.error(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {query_file}")
            return {}
        
        if not os.path.exists(cand_file):
            print(f"âŒ å€™é€‰æ–‡ä»¶ä¸å­˜åœ¨: {cand_file}")
            task_logger.error(f"å€™é€‰æ–‡ä»¶ä¸å­˜åœ¨: {cand_file}")
            return {}
        
        # åŠ è½½M-BEIRæ•°æ®é›†
        print(f"æ­£åœ¨åŠ è½½ {task_name} æ•°æ®é›†...")
        queries, candidates = load_mbeir_data(mbeir_dir, task_name, split)
        
        if len(queries) == 0 or len(candidates) == 0:
            print(f"âŒ æ•°æ®é›†ä¸ºç©º: queries={len(queries)}, candidates={len(candidates)}")
            task_logger.error(f"æ•°æ®é›†ä¸ºç©º: queries={len(queries)}, candidates={len(candidates)}")
            return {}
        
        # åµŒå…¥æ–‡ä»¶è·¯å¾„
        query_save_path = os.path.join(save_dir, f"query_embeddings_{clip_type}.pkl")
        candidate_save_path = os.path.join(save_dir, f"candidate_embeddings_{clip_type}.pkl")
        
        # åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        num_queries = len(queries)
        num_candidates = len(candidates)
        
        # æ ¹æ®æ•°æ®é‡è°ƒæ•´åµŒå…¥æå–çš„æ‰¹å¤„ç†å¤§å°
        if num_queries > 10000 or num_candidates > 50000:
            embedding_batch_size = 16  # å¤§æ•°æ®é›†ç”¨å°æ‰¹æ¬¡
        elif num_queries > 1000 or num_candidates > 10000:
            embedding_batch_size = 32  # ä¸­ç­‰æ•°æ®é›†
        else:
            embedding_batch_size = 64  # å°æ•°æ®é›†ç”¨å¤§æ‰¹æ¬¡
        
        print(f"ä½¿ç”¨åµŒå…¥æå–æ‰¹å¤„ç†å¤§å°: {embedding_batch_size}")
        
        # æå–æŸ¥è¯¢åµŒå…¥
        if not os.path.exists(query_save_path):
            print("æ­£åœ¨æ‰¹å¤„ç†æå–æŸ¥è¯¢åµŒå…¥...")
            query_embeddings, query_metadata = extract_query_embeddings(
                evaluator, queries, img_dir, query_save_path, batch_size=64
            )
        else:
            print("æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„æŸ¥è¯¢åµŒå…¥...")
            with open(query_save_path, 'rb') as f:
                query_embeddings, query_metadata = pickle.load(f)
            print(f"ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢åµŒå…¥: {query_embeddings.shape}")
        
        # æå–å€™é€‰åµŒå…¥
        if not os.path.exists(candidate_save_path):
            print("æ­£åœ¨æ‰¹å¤„ç†æå–å€™é€‰åµŒå…¥...")
            candidate_embeddings, candidate_metadata = extract_candidate_embeddings(
                evaluator, candidates, img_dir, candidate_save_path, batch_size=embedding_batch_size
            )
        else:
            print("æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„å€™é€‰åµŒå…¥...")
            with open(candidate_save_path, 'rb') as f:
                candidate_embeddings, candidate_metadata = pickle.load(f)
            print(f"ä»æ–‡ä»¶åŠ è½½å€™é€‰åµŒå…¥: {candidate_embeddings.shape}")
        
        torch.cuda.empty_cache()
        print("å¼€å§‹è¯„ä¼°æ£€ç´¢æ€§èƒ½...")
        query_embeddings = query_embeddings.cpu().detach()
        candidate_embeddings = candidate_embeddings.cpu().detach()
        
        # æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        num_queries = len(query_embeddings)
        num_candidates = len(candidate_embeddings)
        
        # æ›´æ™ºèƒ½çš„æ‰¹å¤„ç†å¤§å°è°ƒæ•´ç­–ç•¥
        # è€ƒè™‘GPUå†…å­˜é™åˆ¶å’Œè®¡ç®—æ•ˆç‡
        total_pairs = num_queries * num_candidates
        
        if total_pairs > 100000000:  # 1äº¿å¯¹ï¼Œè¶…å¤§æ•°æ®é›†
            batch_size = 8
        elif total_pairs > 50000000:  # 5åƒä¸‡å¯¹ï¼Œå¤§æ•°æ®é›†
            batch_size = 16
        elif total_pairs > 10000000:  # 1åƒä¸‡å¯¹ï¼Œä¸­å¤§æ•°æ®é›†
            batch_size = 32
        elif total_pairs > 1000000:   # 100ä¸‡å¯¹ï¼Œä¸­ç­‰æ•°æ®é›†
            batch_size = 64
        else:
            batch_size = 128  # å°æ•°æ®é›†å¯ä»¥ç”¨æ›´å¤§æ‰¹æ¬¡
            
        print(f"æ£€æµ‹åˆ°æŸ¥è¯¢xå€™é€‰={num_queries}x{num_candidates}={total_pairs:,}, ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        results = evaluate_mbeir_retrieval(
            query_embeddings, 
            candidate_embeddings, 
            query_metadata,
            candidate_metadata,
            k_values=[1, 5, 10, 20],
            batch_size=batch_size
        )
        
        # è·å–top-5å€™é€‰é¡¹ç”¨äºrerank
        print("æ­£åœ¨è·å–top-5å€™é€‰é¡¹ç”¨äºrerank...")
        topk_results = get_topk_candidates_for_rerank(
            query_embeddings=query_embeddings,
            candidate_embeddings=candidate_embeddings,
            query_metadata=query_metadata,
            candidate_metadata=candidate_metadata,
            queries=queries,
            candidates=candidates,
            img_dir=img_dir,
            k=5,
            batch_size=batch_size
        )
        
        # ä¿å­˜top-kç»“æœ
        topk_save_path = os.path.join(save_dir, f"topk_for_rerank_{clip_type}.json")
        save_topk_for_rerank(topk_results, topk_save_path)
        
        # ä¿å­˜ç»“æœ
        results_path = os.path.join(save_dir, f"evaluation_results_{clip_type}.json")
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"âœ… ä»»åŠ¡ {task_name} è¯„ä¼°å®Œæˆ")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # è®°å½•è¯¦ç»†ç»“æœ
        task_logger.info(f"ä»»åŠ¡: {task_name}")
        task_logger.info(f"æ•°æ®åˆ†å‰²: {split}")
        task_logger.info(f"CLIPæ¨¡å‹: {clip_type}")
        task_logger.info(f"æŸ¥è¯¢æ•°é‡: {len(query_embeddings)}")
        task_logger.info(f"å€™é€‰æ•°é‡: {len(candidate_embeddings)}")
        
        for metric, value in results.items():
            task_logger.info(f"{metric}: {value:.4f}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡ {task_name} è¯„ä¼°å¤±è´¥: {e}")
        task_logger.error(f"è¯„ä¼°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return {}
    
    finally:
        # æ¸…ç†èµ„æº
        evaluator.clear_cache()
        # å…³é—­logger handlers
        for handler in task_logger.handlers[:]:
            handler.close()
            task_logger.removeHandler(handler)


def main():
    """ä¸»è¯„ä¼°æµç¨‹ - è‡ªåŠ¨éå†æ‰€æœ‰æ•°æ®é›†"""
    # é…ç½®å‚æ•°
    device = "cuda:6"
    clip_type = "Long-CLIP"  # æˆ– "Long-CLIP"
    
    # M-BEIRæ•°æ®é›†è·¯å¾„å’Œä»»åŠ¡é…ç½®
    mbeir_dir = "/root/dws/MCS/Datasets/M-BEIR"
    img_dir = "/root/dws/MCS/Datasets/M-BEIR"  # å›¾åƒåŸºç¡€ç›®å½•

    available_tasks = [
 
        # "mbeir_cirr_task7",
        # "mbeir_fashioniq_task7",
        "mbeir_webqa_task2",
        # "mbeir_edis_task2",
        # "mbeir_infoseek_task6",
        # "mbeir_oven_task8",
        ""
    ]
    
    split = "test"
    
    # åˆ›å»ºæ€»ä½“ç»“æœä¿å­˜ç›®å½•
    overall_results_dir = "/root/dws/MCS/Codes_MBEIR/overall_results"
    os.makedirs(overall_results_dir, exist_ok=True)
    
    # è®¾ç½®æ€»ä½“æ—¥å¿—
    overall_log_path = os.path.join(overall_results_dir, f"overall_{clip_type}_evaluation.log")
    logging.basicConfig(
        filename=overall_log_path, 
        filemode='w', 
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print(f"å¼€å§‹è‡ªåŠ¨è¯„ä¼°æ‰€æœ‰M-BEIRæ•°æ®é›†")
    print(f"CLIPæ¨¡å‹: {clip_type}")
    print(f"æ€»å…± {len(available_tasks)} ä¸ªä»»åŠ¡")
    print(f"æ€»ä½“æ—¥å¿—å°†ä¿å­˜åˆ°: {overall_log_path}")
    
    # å­˜å‚¨æ‰€æœ‰ä»»åŠ¡çš„ç»“æœ
    all_results = {}
    successful_tasks = []
    failed_tasks = []
    
    # éå†æ‰€æœ‰ä»»åŠ¡
    for i, task_name in enumerate(available_tasks, 1):
        print(f"\nè¿›åº¦: [{i}/{len(available_tasks)}]")
        
        try:
            # è¯„ä¼°å•ä¸ªä»»åŠ¡
            task_results = evaluate_single_task(
                task_name=task_name,
                clip_type=clip_type,
                device=device,
                mbeir_dir=mbeir_dir,
                img_dir=img_dir,
                split=split
            )
            
            if task_results:
                all_results[task_name] = task_results
                successful_tasks.append(task_name)
                logging.info(f"ä»»åŠ¡ {task_name} è¯„ä¼°æˆåŠŸ")
                
                # æ‰“å°ç®€è¦ç»“æœ
                print(f"ä»»åŠ¡ {task_name} ç»“æœ:")
                for metric, value in task_results.items():
                    print(f"  {metric}: {value:.4f}")
            else:
                failed_tasks.append(task_name)
                logging.error(f"ä»»åŠ¡ {task_name} è¯„ä¼°å¤±è´¥")
                
        except Exception as e:
            print(f"âŒ ä»»åŠ¡ {task_name} å‡ºç°å¼‚å¸¸: {e}")
            failed_tasks.append(task_name)
            logging.error(f"ä»»åŠ¡ {task_name} å¼‚å¸¸: {e}")
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    all_results_path = os.path.join(overall_results_dir, f"all_tasks_results_{clip_type}.json")
    with open(all_results_path, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_results = {}
    avg_results_path = None
    if all_results:
        metrics = ['Recall@1', 'Recall@5', 'Recall@10', 'Recall@20']
        
        for metric in metrics:
            values = [results[metric] for results in all_results.values() if metric in results]
            if values:
                avg_results[f"Avg_{metric}"] = sum(values) / len(values)
        
        # ä¿å­˜å¹³å‡ç»“æœ
        if avg_results:
            avg_results_path = os.path.join(overall_results_dir, f"average_results_{clip_type}.json")
            with open(avg_results_path, 'w') as f:
                json.dump(avg_results, f, indent=2)
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"æ‰€æœ‰ä»»åŠ¡è¯„ä¼°å®Œæˆ!")
    print(f"{'='*60}")
    print(f"æˆåŠŸå®Œæˆ: {len(successful_tasks)} ä¸ªä»»åŠ¡")
    print(f"å¤±è´¥ä»»åŠ¡: {len(failed_tasks)} ä¸ªä»»åŠ¡")
    
    if successful_tasks:
        print(f"\nâœ… æˆåŠŸçš„ä»»åŠ¡:")
        for task in successful_tasks:
            print(f"  - {task}")
    
    if failed_tasks:
        print(f"\nâŒ å¤±è´¥çš„ä»»åŠ¡:")
        for task in failed_tasks:
            print(f"  - {task}")
    
    if all_results:
        print(f"\nğŸ“Š å¹³å‡æ€§èƒ½:")
        for metric, value in avg_results.items():
            print(f"  {metric}: {value:.4f}")
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"  - æ‰€æœ‰ç»“æœ: {all_results_path}")
    if avg_results_path:
        print(f"  - å¹³å‡ç»“æœ: {avg_results_path}")
    print(f"  - æ€»ä½“æ—¥å¿—: {overall_log_path}")
    
    # è®°å½•æ€»ç»“åˆ°æ—¥å¿—
    logging.info(f"è¯„ä¼°æ€»ç»“:")
    logging.info(f"æˆåŠŸä»»åŠ¡: {len(successful_tasks)}")
    logging.info(f"å¤±è´¥ä»»åŠ¡: {len(failed_tasks)}")
    logging.info(f"æˆåŠŸä»»åŠ¡åˆ—è¡¨: {successful_tasks}")
    logging.info(f"å¤±è´¥ä»»åŠ¡åˆ—è¡¨: {failed_tasks}")
    
    if avg_results:
        logging.info(f"å¹³å‡æ€§èƒ½:")
        for metric, value in avg_results.items():
            logging.info(f"{metric}: {value:.4f}")

if __name__ == "__main__":
    main()
